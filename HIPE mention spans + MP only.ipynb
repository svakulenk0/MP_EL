{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage: \n",
    "* Run on the same node as Elasticsearch (for now)\n",
    "* **Specify input and output paths in the config dictionary below**\n",
    "* Relax and hope for the best – it's terribly slow (I used parallel notebooks to get the solution ready on time; a smarter way to speed it up will be used in the future)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'DATA_FILE': '../HIPE_2020/data/training-v1.1/en/HIPE-data-v1.1-dev-en.tsv',\n",
    "    'OUTPUT_FILE': 'mp_bundle2_en_2.tsv'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FILE=config['DATA_FILE']\n",
    "OUTPUT_FILE=config['OUTPUT_FILE']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0.0: prepare the data\n",
    "\n",
    "The input data should be in CLEF HIPE format and contain entity mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.data_processing import read_data_to_dfs_sentences, merge_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16190135f0bd469a81d59ceaa5a8908f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=33174.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "raw_dfs = read_data_to_dfs_sentences(DATA_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = merge_dfs(raw_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TOKEN</th>\n",
       "      <th>NE-COARSE-LIT</th>\n",
       "      <th>NE-COARSE-METO</th>\n",
       "      <th>NE-FINE-LIT</th>\n",
       "      <th>NE-FINE-METO</th>\n",
       "      <th>NE-FINE-COMP</th>\n",
       "      <th>NE-NESTED</th>\n",
       "      <th>NEL-LIT</th>\n",
       "      <th>NEL-METO</th>\n",
       "      <th>MISC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [TOKEN, NE-COARSE-LIT, NE-COARSE-METO, NE-FINE-LIT, NE-FINE-METO, NE-FINE-COMP, NE-NESTED, NEL-LIT, NEL-METO, MISC]\n",
       "Index: []"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0.1: prepare Elasticsearch for candidate extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'ilps-cn007',\n",
       " 'cluster_name': 'elasticsearch',\n",
       " 'cluster_uuid': 'vU7MO42lR2KzA6bPvSP_pA',\n",
       " 'version': {'number': '7.7.0',\n",
       "  'build_flavor': 'default',\n",
       "  'build_type': 'tar',\n",
       "  'build_hash': '81a1e9eda8e6183f5237786246f6dced26a10eaf',\n",
       "  'build_date': '2020-05-12T02:01:37.602180Z',\n",
       "  'build_snapshot': False,\n",
       "  'lucene_version': '8.5.1',\n",
       "  'minimum_wire_compatibility_version': '6.8.0',\n",
       "  'minimum_index_compatibility_version': '6.0.0-beta1'},\n",
       " 'tagline': 'You Know, for Search'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "es = Elasticsearch(timeout=30, max_retries=10, retry_on_timeout=True)\n",
    "es.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0.2: prepare HDT for message passing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "\n",
    "from hdt import HDTDocument, TripleComponentRole\n",
    "\n",
    "# from settings import *\n",
    "\n",
    "data_path = './data/wikidata-disambig-train.json'\n",
    "rdfsLabelURI = 'http://www.w3.org/2000/01/rdf-schema#label'\n",
    "\n",
    "hdt_file = 'wikidata20200309.hdt'\n",
    "hdt_path = '/ivi/ilps/personal/vprovat/'\n",
    "\n",
    "PREFIX_E = 'http://www.wikidata.org/entity/'\n",
    "namespace = 'predef-wikidata2020-03-all'\n",
    "kg = HDTDocument(hdt_path+hdt_file)\n",
    "predicates_ids = []\n",
    "kg.configure_hops(1, predicates_ids, namespace, True, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: searching for candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.elastic_getters import wikidata_search_, wikidata_search_precise, wikidata_search_fuzzy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_candidates(es, entity):\n",
    "    '''\n",
    "    Searches for an entity in Elasticsearch.\n",
    "    Is very much suboptimal and will be updated in the future – now left as it is for reproducibility\n",
    "    '''\n",
    "    \n",
    "    hits = wikidata_search_(es, entity) \n",
    "    hits_exact = wikidata_search_precise(es, entity)\n",
    "    hits_fuzzy = wikidata_search_fuzzy(es, entity)\n",
    "        \n",
    "    res_raw = hits_exact + hits + hits_fuzzy  #+ hits_variants\n",
    "    \n",
    "    # removing duplicates:\n",
    "    res_pure = []\n",
    "    seen_Qs = set()\n",
    "    for entry in res_raw:\n",
    "        Q = entry['_source']['label_exact']\n",
    "        if Q not in seen_Qs:\n",
    "            seen_Qs.add(Q)\n",
    "            res_pure.append(entry)\n",
    "            \n",
    "    # removing least relevant results:\n",
    "    res_filtered = []\n",
    "    if not res_pure:\n",
    "        return res_pure # if nothing at all is found (happens very rarely)\n",
    "    \n",
    "    best_score = res_pure[0]['_score'] if res_pure[0]['_score'] else 20\n",
    "    for item in res_pure:\n",
    "        if item['_score'] and item['_score'] > 0.6 * best_score:\n",
    "            res_filtered.append(item)\n",
    "        if not item['_score']: # it means the results were sorted already - just do nothing\n",
    "            res_filtered.append(item)\n",
    "            \n",
    "    return res_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Uncomment the cell below to test candidate search:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get_candidates(es, 'Portugal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: getting the top candidate using MP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_adj_sp(adjacencies, n_entities, include_inverse):\n",
    "    '''\n",
    "    Build adjacency matrix\n",
    "    '''\n",
    "    adj_shape = (n_entities, n_entities)\n",
    "    \n",
    "    # create a single adjacency matrix\n",
    "    adj = sp.csr_matrix((adj_shape))\n",
    "    for edges in adjacencies:\n",
    "        \n",
    "        # split subject (row) and object (col) node URIs\n",
    "        n_edges = len(edges)\n",
    "        row, col = np.transpose(edges)\n",
    "        \n",
    "        # duplicate edges in the opposite direction\n",
    "        if include_inverse:\n",
    "            _row = np.hstack([row, col])\n",
    "            col = np.hstack([col, row])\n",
    "            row = _row\n",
    "            n_edges *= 2\n",
    "        \n",
    "        # create adjacency matrix for this predicate\n",
    "        data = np.ones(n_edges)\n",
    "        adj += sp.csr_matrix((data, (row, col)), shape=adj_shape)\n",
    "    return adj\n",
    "\n",
    "\n",
    "def ned(matched_entities, mention, max_triples=50000000, offset=0, mention_score=100):\n",
    "    \n",
    "    # get all adjacent nodes\n",
    "    all_ids = [v for vs in matched_entities.values() for v in vs]\n",
    "    subgraph1 = kg.compute_hops(all_ids, max_triples, offset)\n",
    "    \n",
    "    # prepare matrices for MP\n",
    "    entity_ids, predicate_ids, adjacencies = subgraph1\n",
    "    n_entities = len(entity_ids)\n",
    "    if predicate_ids:\n",
    "        A = generate_adj_sp(adjacencies, n_entities, include_inverse=True)\n",
    "        \n",
    "        # index entity ids global -> local\n",
    "        entities_dict = {k: v for v, k in enumerate(entity_ids)}\n",
    "\n",
    "        # activate matched entities\n",
    "        row, col, data = [], [], []\n",
    "        for i, span in enumerate(matched_entities):\n",
    "            for entity_id in matched_entities[span]:\n",
    "                if entity_id in entities_dict:\n",
    "                    local_id = entities_dict[entity_id]\n",
    "                    row.append(i)\n",
    "                    col.append(local_id)\n",
    "                    score = 1\n",
    "                    if span == mention:\n",
    "                        score = mention_score\n",
    "                    data.append(score)\n",
    "        x = sp.csr_matrix((data, (row, col)), shape=(len(matched_entities), n_entities))\n",
    "\n",
    "        # MP\n",
    "        y = x @ A\n",
    "        y = sum(y).toarray()[0]\n",
    "\n",
    "        top = np.argwhere(y > mention_score).T.tolist()[0]\n",
    "        activations = defaultdict(int)\n",
    "        if len(top) > 0:\n",
    "            activations1 = np.asarray(entity_ids)[top]\n",
    "\n",
    "            # store the activation values per id answer id\n",
    "            for i, e in enumerate(entity_ids):\n",
    "                if e in activations1:\n",
    "                    activations[e] += y[i]\n",
    "        answers = [{a_id: a_score} for a_id, a_score in sorted(activations.items(), key=lambda item: item[1], reverse=True)[:500] if a_score%mention_score != 0]\n",
    "        answers_ids = [_id for a in answers for _id in a]\n",
    "        answer_uris = []\n",
    "        for a in answers_ids:\n",
    "            uri = kg.global_id_to_string(a, TripleComponentRole.SUBJECT)\n",
    "            if uri:\n",
    "                answer_uris.append(uri)\n",
    "    # filter out answers that do not have labels\n",
    "    top_answers_uris = []\n",
    "\n",
    "    for uri in answer_uris:\n",
    "#         filter out redirects e.g. http://www.wikidata.org/entity/statement/Q271189-081D418E-7709-4074-9864-EDD6B4C46601\n",
    "        if not 'statement' in uri.split('/'):\n",
    "            top_answers_uris.append(uri)\n",
    "\n",
    "    answers = top_answers_uris\n",
    "#     print(\"%d answers found\"%len(answers))\n",
    "    return answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Removing punctuation\n",
    "'''\n",
    "\n",
    "import string\n",
    "table = str.maketrans(dict.fromkeys(string.punctuation)) \n",
    "\n",
    "def remove_punctuation(s):\n",
    "    new_s = s.translate(table)\n",
    "    #also take care of extra whitespaces if they happen\n",
    "    new_s = ' '.join(new_s.split()).strip(' ')\n",
    "    return new_s\n",
    "\n",
    "'''\n",
    "Replacing punctuation with whitespaces - same as above basically\n",
    "'''\n",
    "\n",
    "translator = str.maketrans(string.punctuation + '’', ' '*(len(string.punctuation)+1))\n",
    "def replace_punctuation_with_spaces(s):\n",
    "    new_s = s.translate(translator)\n",
    "    #also take care of extra whitespaces if they happen\n",
    "    new_s = ' '.join(new_s.split()).strip(' ')\n",
    "    return new_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.data_processing import df_to_sentence, extract_entity_mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_candidates_for_all_mentions(mentions, es, top=10):\n",
    "    top_entities = {} # context entities\n",
    "    candidate_entities = []\n",
    "    top_candidate_uris = {}\n",
    "    for pos, raw_label in mentions:\n",
    "        num_words = len(raw_label.split(\" \"))\n",
    "        label = df_to_sentence(df[pos:pos+num_words])\n",
    "        m = label.lower()\n",
    "        results = get_candidates(es, m)[:top]\n",
    "\n",
    "        entity_ids = []\n",
    "        entity_uris = []\n",
    "        for entity in results:\n",
    "            entity_uri = entity['_source']['uri']\n",
    "            entity_uris.append(entity_uri)\n",
    "            entity_id = kg.string_to_global_id(entity_uri, TripleComponentRole.OBJECT)\n",
    "            entity_ids.append(entity_id)\n",
    "        top_entities[m] = entity_ids\n",
    "        top_candidate_uris[m] = entity_uris\n",
    "        \n",
    "    return top_entities, top_candidate_uris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def link_one_mention(mention, cur_candidates, cur_uris, top_entities_):\n",
    "    top_entities = top_entities_.copy()\n",
    "    scores = []\n",
    "    for i, c in enumerate(cur_candidates):\n",
    "#             print('candidate URI:')\n",
    "#             print(candidate_uris[i]) \n",
    "        top_entities[mention] = [c] # entity id in HDT\n",
    "        result_entities = ned(top_entities, mention)\n",
    "#         print('results:')\n",
    "#         print(result_entities)\n",
    "#         print()\n",
    "        scores.append(len(result_entities))\n",
    "    # evaluate: check correct entity id is in the result set\n",
    "#         print('Finished')\n",
    "#         print(candidate_uris[np.argmax(scores)])\n",
    "#         print(correct_id)\n",
    "#         print(scores)\n",
    "\n",
    "    try:\n",
    "        maxind = np.argmax(scores)\n",
    "#         print(maxind)\n",
    "#         print(cur_uris[maxind])\n",
    "        answer = cur_uris[maxind].split('/')[-1]\n",
    "#         print(answer)\n",
    "    except Exception:\n",
    "        answer = 'NIL'\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: performing entity linking for all sentences in our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3a6f43b17ef4fe49695d4fbbdb0c476",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1084.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dfs_with_links = []\n",
    "for i, df in enumerate(tqdm(dfs)):\n",
    "    # Handling word wrapping and NoSpaceAfter flags while preserving the data format:\n",
    "    sentence = df_to_sentence(df)\n",
    "#     print(sentence)\n",
    "    mentions = extract_entity_mentions(df)\n",
    "#     print(mentions)\n",
    "    all_candidates, candidate_uris = get_top_candidates_for_all_mentions(mentions, es)\n",
    "#     print(all_candidates)\n",
    "    new_tokens = sentence.split(' ')\n",
    "    \n",
    "    # Entity linking starts:\n",
    "    df_with_links = df.copy()\n",
    "    for pos, raw_label in mentions:\n",
    "        num_words = len(raw_label.split(\" \"))\n",
    "        label = df_to_sentence(df[pos:pos+num_words]) # in case the label has word wrapping too\n",
    "        \n",
    "        num_removed_spaces = sum(1 for item in df['MISC'].tolist()[:pos] if item == 'NoSpaceAfter')\n",
    "        pos_in_sentence = pos-num_removed_spaces\n",
    "        \n",
    "        answer = link_one_mention(label.lower(), all_candidates[label.lower()], candidate_uris[label.lower()],\n",
    "                                  all_candidates)\n",
    "        \n",
    "        # Adding the links\n",
    "        cur_pos = pos\n",
    "        while cur_pos - pos < num_words:\n",
    "            df_with_links['NEL-LIT'][cur_pos] = answer\n",
    "            df_with_links['NEL-METO'][cur_pos] = answer\n",
    "            cur_pos += 1\n",
    "            \n",
    "    dfs_with_links.append(df_with_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TOKEN</th>\n",
       "      <th>NE-COARSE-LIT</th>\n",
       "      <th>NE-COARSE-METO</th>\n",
       "      <th>NE-FINE-LIT</th>\n",
       "      <th>NE-FINE-METO</th>\n",
       "      <th>NE-FINE-COMP</th>\n",
       "      <th>NE-NESTED</th>\n",
       "      <th>NEL-LIT</th>\n",
       "      <th>NEL-METO</th>\n",
       "      <th>MISC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>But</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>nothing</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>can</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>measure</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>the</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>spir</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "      <td>NoSpaceAfter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>¬</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "      <td>EndOfLine|NoSpaceAfter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>itual</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>power</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>for</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>good</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>of</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>selfless</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "      <td>NoSpaceAfter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>,</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "      <td>EndOfLine|NoSpaceAfter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>silent</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>right</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>thinking</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "      <td>NoSpaceAfter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>.</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "      <td>EndOfLine|NoSpaceAfter</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       TOKEN NE-COARSE-LIT NE-COARSE-METO NE-FINE-LIT NE-FINE-METO  \\\n",
       "0        But             O              O           O            O   \n",
       "1    nothing             O              O           O            O   \n",
       "2        can             O              O           O            O   \n",
       "3    measure             O              O           O            O   \n",
       "4        the             O              O           O            O   \n",
       "5       spir             O              O           O            O   \n",
       "6          ¬             O              O           O            O   \n",
       "7      itual             O              O           O            O   \n",
       "8      power             O              O           O            O   \n",
       "9        for             O              O           O            O   \n",
       "10      good             O              O           O            O   \n",
       "11        of             O              O           O            O   \n",
       "12  selfless             O              O           O            O   \n",
       "13         ,             O              O           O            O   \n",
       "14    silent             O              O           O            O   \n",
       "15     right             O              O           O            O   \n",
       "16  thinking             O              O           O            O   \n",
       "17         .             O              O           O            O   \n",
       "\n",
       "   NE-FINE-COMP NE-NESTED NEL-LIT NEL-METO                    MISC  \n",
       "0             O         O       _        _                       _  \n",
       "1             O         O       _        _                       _  \n",
       "2             O         O       _        _                       _  \n",
       "3             O         O       _        _                       _  \n",
       "4             O         O       _        _                       _  \n",
       "5             O         O       _        _            NoSpaceAfter  \n",
       "6             O         O       _        _  EndOfLine|NoSpaceAfter  \n",
       "7             O         O       _        _                       _  \n",
       "8             O         O       _        _                       _  \n",
       "9             O         O       _        _                       _  \n",
       "10            O         O       _        _                       _  \n",
       "11            O         O       _        _                       _  \n",
       "12            O         O       _        _            NoSpaceAfter  \n",
       "13            O         O       _        _  EndOfLine|NoSpaceAfter  \n",
       "14            O         O       _        _                       _  \n",
       "15            O         O       _        _                       _  \n",
       "16            O         O       _        _            NoSpaceAfter  \n",
       "17            O         O       _        _  EndOfLine|NoSpaceAfter  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_with_links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: saving the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.data_processing import write_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_results(dfs_with_links, OUTPUT_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nel",
   "language": "python",
   "name": "nel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
